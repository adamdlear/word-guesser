{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import en_core_web_sm\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import datasets\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import TFAutoModelForSequenceClassification\n",
    "from transformers import DataCollatorWithPadding\n",
    "from transformers import create_optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = en_core_web_sm.load()\n",
    "# nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Move Wordnet Synsets to Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordnet_nouns = wn.all_eng_synsets('n')\n",
    "wordnet_verbs = wn.all_eng_synsets('v')\n",
    "wordnet_adjs = wn.all_eng_synsets('a')\n",
    "wordnet_advs = wn.all_eng_synsets('r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, truncate=True, use_fast=False)\n",
    "batch_size = 16\n",
    "\n",
    "max_input_length = 200\n",
    "max_target_length = 200\n",
    "\n",
    "# def preprocess_function(examples):\n",
    "#     model_inputs = tokenizer(\n",
    "#         examples[\"definition\"],\n",
    "#         max_length=max_input_length,\n",
    "#         truncation=True,\n",
    "#         padding='longest'\n",
    "#     )\n",
    "#     labels = tokenizer(\n",
    "#         examples[\"name\"],\n",
    "#         max_length=max_target_length,\n",
    "#         truncation=True,\n",
    "#         padding='longest'\n",
    "#     )\n",
    "#     model_inputs[\"labels\"] = tf.constant(labels[\"input_ids\"])\n",
    "#     return model_inputs\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples['definition'], truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 65692/65692 [00:15<00:00, 4253.15 examples/s]\n",
      "Map: 100%|██████████| 16423/16423 [00:03<00:00, 4241.42 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def create_dataset(wordnet_data):\n",
    "    dictionary = {\n",
    "        'name': [],\n",
    "        'definition': [],\n",
    "    }\n",
    "\n",
    "    for s in wordnet_data:\n",
    "        dictionary['name'].append(s.name())\n",
    "        dictionary['definition'].append(s.definition())\n",
    "\n",
    "    dataset = datasets.Dataset.from_dict(dictionary)\n",
    "    return dataset\n",
    "\n",
    "nouns = create_dataset(wordnet_nouns) \\\n",
    "            .train_test_split(test_size=.2)\n",
    "encoded_nouns = nouns.map(preprocess_function, batched=True) \\\n",
    "            # .select_columns(['input_ids', 'attention_mask', 'labels']) \\\n",
    "# nouns = nouns.remove_columns(['name', 'definition'])\n",
    "# verbs = create_dataset(wordnet_verbs) \\\n",
    "#             .train_test_split(test_size=.2) \\\n",
    "#             .map(preprocess_function, batched=True) \\\n",
    "#             .select_columns(['input_ids', 'attention_mask', 'labels'])\n",
    "# adjs = create_dataset(wordnet_adjs) \\\n",
    "#             .train_test_split(test_size=.2) \\\n",
    "#             .map(preprocess_function, batched=True) \\\n",
    "#             .select_columns(['input_ids', 'attention_mask', 'labels'])\n",
    "# advs = create_dataset(wordnet_advs) \\\n",
    "#             .train_test_split(test_size=.2) \\\n",
    "#             .map(preprocess_function, batched=True) \\\n",
    "#             .select_columns(['input_ids', 'attention_mask', 'labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer, return_tensors=\"tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFDistilBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "in user code:\n\n    File \"/Users/adamlear/Desktop/portfolio/projects/word-guesser/.env/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"/Users/adamlear/Desktop/portfolio/projects/word-guesser/.env/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/Users/adamlear/Desktop/portfolio/projects/word-guesser/.env/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"/Users/adamlear/Desktop/portfolio/projects/word-guesser/.env/lib/python3.10/site-packages/transformers/modeling_tf_utils.py\", line 1678, in train_step\n        self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\n    File \"/Users/adamlear/Desktop/portfolio/projects/word-guesser/.env/lib/python3.10/site-packages/keras/src/optimizers/optimizer.py\", line 543, in minimize\n        grads_and_vars = self.compute_gradients(loss, var_list, tape)\n    File \"/Users/adamlear/Desktop/portfolio/projects/word-guesser/.env/lib/python3.10/site-packages/keras/src/optimizers/optimizer.py\", line 276, in compute_gradients\n        grads = tape.gradient(loss, var_list)\n\n    TypeError: Argument `target` should be a list or nested structure of Tensors, Variables or CompositeTensors to be differentiated, but received None.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/adamlear/Desktop/portfolio/projects/word-guesser/exploration/testing3.ipynb Cell 8\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/adamlear/Desktop/portfolio/projects/word-guesser/exploration/testing3.ipynb#X30sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m optimizer, schedule \u001b[39m=\u001b[39m create_optimizer(init_lr\u001b[39m=\u001b[39m\u001b[39m2e-5\u001b[39m, num_warmup_steps\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, num_train_steps\u001b[39m=\u001b[39mtotal_train_steps)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/adamlear/Desktop/portfolio/projects/word-guesser/exploration/testing3.ipynb#X30sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m noun_model\u001b[39m.\u001b[39mcompile(optimizer\u001b[39m=\u001b[39moptimizer)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/adamlear/Desktop/portfolio/projects/word-guesser/exploration/testing3.ipynb#X30sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m noun_model\u001b[39m.\u001b[39;49mfit(tf_train_dataset, validation_data\u001b[39m=\u001b[39;49mtf_validation_dataset, epochs\u001b[39m=\u001b[39;49m\u001b[39m3\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/adamlear/Desktop/portfolio/projects/word-guesser/exploration/testing3.ipynb#X30sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m \u001b[39m# noun_model.save_pretrained(\"models/nouns-model\")\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/portfolio/projects/word-guesser/.env/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/var/folders/9g/wrdq3v9s7dx67f6t3vh6yj580000gn/T/__autograph_generated_filem_zc0gqv.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(step_function), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m), ag__\u001b[39m.\u001b[39mld(iterator)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/portfolio/projects/word-guesser/.env/lib/python3.10/site-packages/transformers/modeling_tf_utils.py:1678\u001b[0m, in \u001b[0;36mTFPreTrainedModel.train_step\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1675\u001b[0m         loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompiled_loss(y, y_pred, sample_weight, regularization_losses\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlosses)\n\u001b[1;32m   1677\u001b[0m \u001b[39m# Run backwards pass.\u001b[39;00m\n\u001b[0;32m-> 1678\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptimizer\u001b[39m.\u001b[39;49mminimize(loss, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrainable_variables, tape\u001b[39m=\u001b[39;49mtape)\n\u001b[1;32m   1680\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompiled_metrics\u001b[39m.\u001b[39mupdate_state(y, y_pred, sample_weight)\n\u001b[1;32m   1681\u001b[0m \u001b[39m# Collect metrics to return\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: in user code:\n\n    File \"/Users/adamlear/Desktop/portfolio/projects/word-guesser/.env/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"/Users/adamlear/Desktop/portfolio/projects/word-guesser/.env/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/Users/adamlear/Desktop/portfolio/projects/word-guesser/.env/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"/Users/adamlear/Desktop/portfolio/projects/word-guesser/.env/lib/python3.10/site-packages/transformers/modeling_tf_utils.py\", line 1678, in train_step\n        self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\n    File \"/Users/adamlear/Desktop/portfolio/projects/word-guesser/.env/lib/python3.10/site-packages/keras/src/optimizers/optimizer.py\", line 543, in minimize\n        grads_and_vars = self.compute_gradients(loss, var_list, tape)\n    File \"/Users/adamlear/Desktop/portfolio/projects/word-guesser/.env/lib/python3.10/site-packages/keras/src/optimizers/optimizer.py\", line 276, in compute_gradients\n        grads = tape.gradient(loss, var_list)\n\n    TypeError: Argument `target` should be a list or nested structure of Tensors, Variables or CompositeTensors to be differentiated, but received None.\n"
     ]
    }
   ],
   "source": [
    "noun_model = TFAutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=16000)\n",
    "\n",
    "# tf_train_dataset = nouns['train'].to_tf_dataset(\n",
    "#     columns=['attention_mask', 'input_ids', 'label'],\n",
    "#     shuffle=True,\n",
    "#     batch_size=8,\n",
    "#     collate_fn=data_collator\n",
    "# )\n",
    "\n",
    "# tf_validation_dataset = nouns['train'].to_tf_dataset(\n",
    "#     columns=['attention_mask', 'input_ids', 'label'],\n",
    "#     shuffle=False,\n",
    "#     batch_size=8,\n",
    "#     collate_fn=data_collator\n",
    "# )\n",
    "\n",
    "tf_train_dataset = noun_model.prepare_tf_dataset(\n",
    "    encoded_nouns['train'],\n",
    "    shuffle=True,\n",
    "    batch_size=16,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "tf_validation_dataset = noun_model.prepare_tf_dataset(\n",
    "    encoded_nouns['test'],\n",
    "    shuffle=False,\n",
    "    batch_size=16,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "batch_size = 8\n",
    "num_epochs = 5\n",
    "batches_per_epoch = len(nouns['train']) // batch_size\n",
    "total_train_steps = int(batches_per_epoch * num_epochs)\n",
    "optimizer, schedule = create_optimizer(init_lr=2e-5, num_warmup_steps=0, num_train_steps=total_train_steps)\n",
    "\n",
    "noun_model.compile(optimizer=optimizer)\n",
    "noun_model.fit(tf_train_dataset, validation_data=tf_validation_dataset, epochs=3)\n",
    "# noun_model.save_pretrained(\"models/nouns-model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize Word Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Move Wordnet Synsets to TensorFlow Data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
